---
title: "Propensity Score Analysis (PSA) -- Day 2"
# subtitle: "why we use it and what does it provide"
author: 
    Chaochen Wang (CWAN) <br>
    Thomas Laurent (TLAU)
date: "2020-2-28 14:30~15:30 @CSS"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      countdown: 60000
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---



# Recap

We discussed about:

- the causal inference framework under which that PSA was designed for; 

- the assumptions required when consider using the PSA;

    - No interference, Consistency, Conditional Exchangeability

- ATE calculated through a stratification procedure. 

---
class: middle

# Today 

We will try to cover: 

- Adjusting for PS in the model; 

- Matching participants by PS; 

- Inversely weighting the participants by their PS;

<!-- - Cautions and best guidelines to follow when reporting studies used PS.  -->

---
class: middle

## Regression Adjustment

- Another approach of using PS is to **adjust** for the propensity score in a regression model.

$$E\{Y|X, p(\mathbf{C})\} = \alpha + \color{red}{\beta} X + \gamma p(\mathbf{C})$$


- $\color{red}{\beta}$ potentially has a **causal (conditional) interpretation**

    - because if conditional exchangeability holds given $\mathbf{C}$ then it also holds given $p(\mathbf{C})$
    
- $\gamma$ is now one-dimensional so, **finite sample bias** would no longer be a problem.


  
---
class: inverse
background-image: url("./fig/adjustment.png")
background-position: 50% 50%
background-size: contain


???

finite sample bias is no longer a concern for propensity score adjustment as the number of covariates increases. 


---
class: middle

## Matching on the estimated PS

Matching on the PS means we take one patient who recieved RFA and find one (or more than one) match for him/her **with replacement** from those who recieved standard surgery but with a similar value of PS.

- To estimate **average treatment effect in the treated (ATT)**;


- If we start matching with those who recieved standard surgery, this will be an estimate of the **average treatment effect in the untreated (ATU)**.


---
class: middle

## Matching methods

- nearest neightbour matching

- within calipers method (Mahalanobis metric matching)

- etc.

- Problem of **poor overlap (or lack of positivity)** is immediately flagged up when some individuals fail to be mached. 

---
class: middle

## Balance diagnostics after matching

- [Standardized mean difference (SMD)](https://cran.r-project.org/web/packages/tableone/vignettes/smd.html) is mostly used: 

$$SMD = \frac{\bar{X_1} - \bar{X_2}}{\sqrt{(\hat{\sigma_1}^2 + \hat{\sigma_2}^2)/2}} <0.1$$

- For dichotomous variables and categorical variable, see reference <sup>1</sup>: 

.between[
Yang DS, Dalton JE. A Unified Approach to Measuring the Effect Size Between Two Groups Using SAS. SAS Global Forum 2012. paper 335
]

---
class: middle

## Inverse probability weighting (1)

- The propensity scores we have calculated are the **conditional probabilities** - based on all of the confounders we have found and included - that the individuals will be exposed. 

- So there are some individuals, who, condition on their confounders, are more or less likely to be exposed. 

---
class: middle 

## Inverse probability weighting (2)

- If we found that someone in the study, condition on her/his all available confounders, was **unlikely** to be exposed to the intervention (treatment), $p(\mathbf{C})$ is small.

--
- We may consider **upweight** him/her, so that (s)he represents him/her and also many other who may like him/her who were unlikely to be unexposed.

--
- Then we will have a re-weighted dataset in which $\mathbf{X}$ (exposure), and $p(\mathbf{C})$ are independent, but everything else is unchanged. 


---
class: middle 

## Simple example (1)

- Suppose the counterfactual data are:

|      Group     | A | A | A | B | B | B | D | D | D |
|:--------------:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| Response $Y(1)$: | 1 | 1 | 1 | 2 | 2 | 2 | 3 | 3 | 3 |
| Response $Y(0)$: | 0 | 0 | 0 | 1 | 1 | 1 | 2 | 2 | 2 |

- Even without fitting any model, we can tell that the average treatment effect (ATE) is 1. 

---
class: middle

## Simple example (2)

- However, we can only observe (in the real world under consistency):

|     Group    |  A |  A |  A |  B |  B |  B |  D |  D |  D |
|:------------:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
|  Exposed $Y$:  |  1 |  1 | NA | NA |  2 | NA |  3 | NA | NA |
| Unexposed $Y$: | NA | NA |  0 |  1 | NA |  1 | NA |  2 |  2 |


- The estimated average treatment effect (ATE) is <br> 7/4 - 6/5 = 0.55, **(hugely biased)**


---
class: middle

## Weighting to reduce bias (IPTW)

- The probability of recieving treatment: 

  - 2/3 in group A; 
  - 1/3 in group B;
  - 1/3 in group D.

- Calculate weighted average <br> (by **1/{Probability of observed treament}**)


.small[
$$\frac{(1 + 1) \times \frac{3}{2} + (2) \times \frac{3}{1} + (3) \times \frac{3}{1}}{\frac{3}{2} + \frac{3}{2} + \frac{3}{1} + \frac{3}{1}} \\ \;\;\;\;- \frac{(0) \times \frac{3}{1} + (1 + 1) \times \frac{3}{2} + (2 + 2) \times \frac{3}{2}}{\frac{3}{1} + \frac{3}{2}+ \frac{3}{2}+ \frac{3}{2}}= 2-1 = 1$$
]

---
class: middle

## **I**nverse **P**robability of **T**reatment **W**eighting (IPTW)

- IPTW successfully removed the bias by creating a "fake" population where we 'observe' each subject at each exposure level.

- It can be proved that IPTW can provide consistent estimators. 

---
class: middle

## IPTW conditioning on binary confounder C (1)

.pull-left[
- Observed data: 

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;border-width:1px;border-style:solid;border-color:#ccc;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;}
.tg .tg-p0ii{background-color:#f9f9f9;font-size:22px;border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-9d8n{font-size:22px;border-color:inherit;text-align:center;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-9d8n"></th>
    <th class="tg-9d8n" colspan="2">Y = 1</th>
    <th class="tg-9d8n" colspan="2">Y = 0</th>
  </tr>
  <tr>
    <td class="tg-9d8n"></td>
    <td class="tg-p0ii">C = 1</td>
    <td class="tg-9d8n">C = 0</td>
    <td class="tg-p0ii">C = 1</td>
    <td class="tg-9d8n">C = 0</td>
  </tr>
  <tr>
    <td class="tg-9d8n">X = 1</td>
    <td class="tg-p0ii">180</td>
    <td class="tg-9d8n">200</td>
    <td class="tg-p0ii">600</td>
    <td class="tg-9d8n">200</td>
  </tr>
  <tr>
    <td class="tg-9d8n">X = 0</td>
    <td class="tg-p0ii">20</td>
    <td class="tg-9d8n">200</td>
    <td class="tg-p0ii">200</td>
    <td class="tg-9d8n">600</td>
  </tr>
</table>
]



.pull-right[
- Table of $\color{red}{P(X|C)}$ 

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;border-width:1px;border-style:solid;border-color:#ccc;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;}
.tg .tg-p0ii{background-color:#f9f9f9;font-size:22px;border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-9d8n{font-size:22px;border-color:inherit;text-align:center;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-9d8n"></th>
    <th class="tg-9d8n" colspan="2">Y = 1</th>
    <th class="tg-9d8n" colspan="2">Y = 0</th>
  </tr>
  <tr>
    <td class="tg-9d8n"></td>
    <td class="tg-p0ii">C = 1</td>
    <td class="tg-9d8n">C = 0</td>
    <td class="tg-p0ii">C = 1</td>
    <td class="tg-9d8n">C = 0</td>
  </tr>
  <tr>
    <td class="tg-9d8n">X = 1</td>
    <td class="tg-p0ii">0.780</td>
    <td class="tg-9d8n">0.333</td>
    <td class="tg-p0ii">0.780</td>
    <td class="tg-9d8n">0.333</td>
  </tr>
  <tr>
    <td class="tg-9d8n">X = 0</td>
    <td class="tg-p0ii">0.220</td>
    <td class="tg-9d8n">0.667</td>
    <td class="tg-p0ii">0.220</td>
    <td class="tg-9d8n">0.667</td>
  </tr>
</table>

]

Where, $0.780 = (180 + 600)/(180 + 600 + 20 + 200)$


---
class: middle 
## IPTW conditioning on binary confounder C (2)

- Dividing each cell count by $\color{red}{P(X|C)}$ 

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;border-width:1px;border-style:solid;border-color:#ccc;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;}
.tg .tg-p0ii{background-color:#f9f9f9;font-size:22px;border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-9d8n{font-size:22px;border-color:inherit;text-align:center;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-9d8n"></th>
    <th class="tg-9d8n" colspan="2">Y = 1</th>
    <th class="tg-9d8n" colspan="2">Y = 0</th>
  </tr>
  <tr>
    <td class="tg-9d8n"></td>
    <td class="tg-p0ii">C = 1</td>
    <td class="tg-9d8n">C = 0</td>
    <td class="tg-p0ii">C = 1</td>
    <td class="tg-9d8n">C = 0</td>
  </tr>
  <tr>
    <td class="tg-9d8n">X = 1</td>
    <td class="tg-p0ii">180/0.78 <br>= 231</td>
    <td class="tg-9d8n">200/0.333 <br>= 600</td>
    <td class="tg-p0ii">600/0.78 <br>= 769</td>
    <td class="tg-9d8n">200/0.333 <br>= 600</td>
  </tr>
  <tr>
    <td class="tg-9d8n">X = 0</td>
    <td class="tg-p0ii">20/0.22 <br>= 91</td>
    <td class="tg-9d8n">200/0.667 <br>= 300</td>
    <td class="tg-p0ii">200/0.22 <br>= 909</td>
    <td class="tg-9d8n">600/0.667 <br>= 900</td>
  </tr>
</table>

---
class: middle
## IPTW conditioning on binary confounder C (3)

- The impact of inverse weighting by $\color{red}{P(X|C)}$ is to create a pseudo(fake)-population where we 'observe' each subject at each exposure level: 

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;border-width:1px;border-style:solid;border-color:#ccc;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;}
.tg .tg-p0ii{background-color:#f9f9f9;font-size:22px;border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-390v{background-color:#f8ff00;font-size:22px;border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-9d8n{font-size:22px;border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-ovs6{font-size:22px;background-color:#f8ff00;border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-9d8n"></th>
    <th class="tg-9d8n" colspan="2">Y = 1</th>
    <th class="tg-9d8n" colspan="2">Y = 0</th>
    <th class="tg-9d8n" colspan="2"> Total </th>
    <!-- <th class="tg-0pky"></th> -->
  </tr>
  <tr>
    <td class="tg-9d8n"></td>
    <td class="tg-p0ii">C = 1</td>
    <td class="tg-9d8n">C = 0</td>
    <td class="tg-p0ii">C = 1</td>
    <td class="tg-9d8n">C = 0</td>
    <td class="tg-390v">C = 1</td>
    <td class="tg-ovs6">C = 0</td>
  </tr>
  <tr>
    <td class="tg-9d8n">X = 1</td>
    <td class="tg-p0ii">231</td>
    <td class="tg-9d8n">600</td>
    <td class="tg-p0ii">769</td>
    <td class="tg-9d8n">600</td>
    <td class="tg-390v">1000</td>
    <td class="tg-ovs6">1200</td>
  </tr>
  <tr>
    <td class="tg-9d8n">X = 0</td>
    <td class="tg-p0ii">91</td>
    <td class="tg-9d8n">300</td>
    <td class="tg-p0ii">909</td>
    <td class="tg-9d8n">900</td>
    <td class="tg-390v">1000</td>
    <td class="tg-ovs6">1200</td>
  </tr>
</table>

---
class: middle

# PS matching and IPTW example - data

- Right heart catheterization data

- We can download the data from : [http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/rhc.csv)

- Data are from ICU patients in 5 hospitals

- **Treatment**: right heart catheterization (rhc) or not

- **Outcome**: death

- **Confounders**: demographics, insurance, disease diagnoses, etc.

???
catheterization: カテーテル
<!-- (https://www.coursera.org/lecture/crash-course-in-causality/data-example-in-r-Ie48W) -->


---
class: middle

## First steps <br>- load packages, read in data

```{r message=FALSE, cache=TRUE}
# load packages
library(tableone)
library(Matching)

# Read in data
load(url("http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/rhc.sav"))
```


---
class: middle

## Second steps - create data set

.small[
```{r}
# create a data set with just these variables, for simplicity
library(tidyverse) # load this package if you have not

dat <- rhc %>% 
  dplyr::select(cat1, sex, death, age, swang1, meanbp1) %>% 
  mutate(ARF = as.numeric(cat1 == "ARF"), 
         CHF = as.numeric(cat1 == "CHF"), 
         Cirr = as.numeric(cat1 == "Cirrhosis"), 
         colcan = as.numeric(cat1 == "Colon Cancer"), 
         Coma = as.numeric(cat1 == "Coma"), 
         COPD = as.numeric(cat1 == "COPD"), 
         lungcan = as.numeric(cat1 == "Lung Cancer"), 
         MOSF = as.numeric(cat1 == "MOSF w/Malignancy"), 
         sepsis = as.numeric(cat1 == "MOSF w/Sepsis"), 
         female = as.numeric(sex == "Female"), 
         died = as.numeric(death == "Yes"), 
         treatment = as.numeric(swang1 == "RHC"), 
         age = as.numeric(age), 
         meanbp1 = as.numeric(meanbp1))

```
]


---
class: middle 

## Continued

.small[
```{r}
# Covariates will be used: 
xvars <- c("ARF", "CHF", "Cirr", "colcan", "Coma", "lungcan", "MOSF", 
           "sepsis", "age", "female", "meanbp1")

```
]

### Create a Table 1

.small[
```{r eval=FALSE}
# Look at a table 1

table1 <- CreateTableOne(vars = xvars, strata = "treatment", 
                         data = dat, test = FALSE)

# include standardized mean difference (SMD)
print(table1, smd = TRUE)

```
]

---
class: middle

### Table 1, unmatched

.small[
```r
                     Stratified by treatment
                      0             1             SMD   
  n                    3551          2184               
  ARF (mean (SD))      0.45 (0.50)   0.42 (0.49)   0.059
  CHF (mean (SD))      0.07 (0.25)   0.10 (0.29)   0.095
* Cirr (mean (SD))     0.05 (0.22)   0.02 (0.15)   0.145
  colcan (mean (SD))   0.00 (0.04)   0.00 (0.02)   0.038
* Coma (mean (SD))     0.10 (0.29)   0.04 (0.20)   0.207
  lungcan (mean (SD))  0.01 (0.10)   0.00 (0.05)   0.095
  MOSF (mean (SD))     0.07 (0.25)   0.07 (0.26)   0.018
* sepsis (mean (SD))   0.15 (0.36)   0.32 (0.47)   0.415
  age (mean (SD))     61.76 (17.29) 60.75 (15.63)  0.061
  female (mean (SD))   0.46 (0.50)   0.41 (0.49)   0.093
* meanbp1 (mean (SD)) 84.87 (38.87) 68.20 (34.24)  0.455
```
]


???

- A few variables showed some imbalanced. (SMD > 0.1)


---
class: middle

### Match

- Greedy matching based on [Mahalanobis distance](https://ja.wikipedia.org/wiki/%E3%83%9E%E3%83%8F%E3%83%A9%E3%83%8E%E3%83%93%E3%82%B9%E8%B7%9D%E9%9B%A2) 

.small[
```{r eval=FALSE}
# do greedy matching
greedymatch <- Match(Tr = dat$treatment, 
                     M = 1, # 1:1
                     X = dat[xvars])

matched <- dat[unlist(greedymatch[c("index.treated", "index.control")]), ]
matchedtab1 <- CreateTableOne(vars = xvars, strata = "treatment", 
                              data = matched, test = FALSE)

print(matchedtab1, smd = TRUE)
```
]

---
class: middle

### Matched data Table 1

.small[
```r
                     Stratified by treatment
                      0             1             SMD   
  n                    2186          2186               
  ARF (mean (SD))      0.42 (0.49)   0.42 (0.49)  <0.001
  CHF (mean (SD))      0.10 (0.29)   0.10 (0.29)  <0.001
  Cirr (mean (SD))     0.02 (0.15)   0.02 (0.15)  <0.001
  colcan (mean (SD))   0.00 (0.02)   0.00 (0.02)  <0.001
  Coma (mean (SD))     0.04 (0.20)   0.04 (0.20)  <0.001
  lungcan (mean (SD))  0.00 (0.05)   0.00 (0.05)  <0.001
  MOSF (mean (SD))     0.07 (0.26)   0.07 (0.26)  <0.001
  sepsis (mean (SD))   0.32 (0.47)   0.32 (0.47)  <0.001
  age (mean (SD))     60.84 (15.54) 60.77 (15.64)  0.005
  female (mean (SD))   0.41 (0.49)   0.41 (0.49)  <0.001
  meanbp1 (mean (SD)) 68.26 (33.23) 68.19 (34.23)  0.002
```
]

---
class: middle

### Outcome analysis -  causal risk difference<br>by simply performing a paired t-test

.small[
```{r echo=FALSE}
# do greedy matching
greedymatch <- Match(Tr = dat$treatment, 
                     M = 1, # 1:1
                     X = dat[xvars])

matched <- dat[unlist(greedymatch[c("index.treated", "index.control")]), ]
matchedtab1 <- CreateTableOne(vars = xvars, strata = "treatment", 
                              data = matched, test = FALSE)
```
]

.small[
```{r}
# outcome analysis
y_trt <- matched$died[matched$treatment == 1]
y_con <- matched$died[matched$treatment == 0]

# pairwise difference
diffy <- y_trt - y_con

# paired t-test
t.test(diffy)
```
]

---
class: middle

### Causal risk difference - summary

- Point estimate: 0.045

    - Difference in probability of death **if everyone received RHC** compared with **if no one recieved RHC** is 0.045 (i.e., higher risk of death in RHC group)

- 95% CI: (0.019, 0.072)

- P-value: < 0.001

- don't take it too seriously, this is just for illustration, we have not controlled for all of the confounders. 

---
class: middle

### McNemar test

.small[
```{r}
# McNemar test

table(y_trt, y_con)

mcnemar.test(matrix(c(994, 493, 394, 305), 2, 2))
```
]


---
class: middle

### McNemar test - summary

- 493 + 394 pairs are discordant

- 493 means when a treated person died and a control person did not

- P value < 0.001 

- Same conclusion from the paried t-test <br>-- treated persons were at higher risk of death


---
class: middle

# IPTW - example using the same data

- load packages, read in data

.small[
```{r message=FALSE, cache=TRUE}
# load packages
library(tableone)
{{library(sandwich)}}
    # for robust variance estimation
{{library(ipw)}}
{{library(survey)}}

# Read in data
load(url("http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/rhc.sav"))
```
]

---
class: middle

## Calculate the propensity score

.small[
```{r echo=FALSE, cache=TRUE}
# create a data set with just these variables, for simplicity
library(tidyverse) # load this package if you have not

dat <- rhc %>% 
  dplyr::select(cat1, sex, death, age, swang1, meanbp1, aps1) %>% 
  mutate(ARF = as.numeric(cat1 == "ARF"), 
         CHF = as.numeric(cat1 == "CHF"), 
         Cirr = as.numeric(cat1 == "Cirrhosis"), 
         colcan = as.numeric(cat1 == "Colon Cancer"), 
         Coma = as.numeric(cat1 == "Coma"), 
         COPD = as.numeric(cat1 == "COPD"), 
         lungcan = as.numeric(cat1 == "Lung Cancer"), 
         MOSF = as.numeric(cat1 == "MOSF w/Malignancy"), 
         sepsis = as.numeric(cat1 == "MOSF w/Sepsis"), 
         female = as.numeric(sex == "Female"), 
         died = as.numeric(death == "Yes"), 
         treatment = as.numeric(swang1 == "RHC"), 
         age = as.numeric(age), 
         meanbp1 = as.numeric(meanbp1), 
         aps = as.numeric(aps1))

```
]

.small[
```{r}
# propensity score model
psmodel <- glm(treatment ~ age + female + meanbp1 + ARF + 
                 CHF + Cirr + colcan + Coma + lungcan + 
                 MOSF + sepsis + aps, 
               family = binomial(link = "logit"), 
               data = dat)

# value of propensity score for each subject
ps <- predict(psmodel, type = "response")
```
]

---
class: middle

## Results from the PS model

.small[
```r
Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept) -1.946015   0.232129   -8.38  < 2e-16 ***
*age         -0.003047   0.001746   -1.74  0.08101 .  
*female      -0.139077   0.059014   -2.36  0.01844 *  
*meanbp1     -0.007517   0.000871   -8.63  < 2e-16 ***
*ARF          1.225293   0.149551    8.19  2.5e-16 ***
*CHF          1.890564   0.173569   10.89  < 2e-16 ***
Cirr         0.433406   0.220337    1.97  0.04918 *  
colcan       0.048157   1.124289    0.04  0.96583    
*Coma         0.684254   0.187833    3.64  0.00027 ***
lungcan      0.198460   0.505500    0.39  0.69461    
*MOSF         1.017780   0.180716    5.63  1.8e-08 ***
*sepsis       1.840246   0.156159   11.78  < 2e-16 ***
*aps          0.018236   0.001729   10.55  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7621.4  on 5734  degrees of freedom
Residual deviance: 6869.2  on 5722  degrees of freedom
AIC: 6895

Number of Fisher Scoring iterations: 4
```
]

???
You probably will be interested on which variables are associated with receiving the treatment in the data.

- age negative coefficient


---
class: inverse
background-image: url("./fig/rhcpreweighting.png")
background-position: 50% 50%
background-size: contain

```{r psfigure, echo=FALSE, fig.height=6, fig.width=8, fig.cap='', fig.align='center', out.width='100%', message=FALSE, warning=FALSE, eval=FALSE}
ggthemr::ggthemr('fresh', layout = 'scientific')
dat$Score <- ps
dat %>%
  ggplot(aes(x = Score, y= ..density.., fill = as.factor(treatment))) + 
  geom_histogram(position = "identity", color = "black", alpha = 0.5) +
  geom_density(alpha = 0.2) +
  theme(axis.title = element_text(size = 17), axis.text = element_text(size = 14),
        axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank()) +
  ggtitle("Density and histogram of the estimated \npropensity score in the two treatment groups.") +
 theme(title = element_text(size = 20),
       plot.title = element_text(hjust=0.5),
  legend.text = element_text(size = 19), 
  legend.title = element_text(size = 18),
  legend.position = "bottom", legend.direction = "horizontal") +
  labs(fill = "Treatment Methods") + 
  scale_fill_discrete(labels = c("no RHC", "RHC")) 
ggthemr::ggthemr_reset()
```


???
good overlapping between two treatment groups


---
class: middle

## Create weights and check balance

.small[
```{r}
# Create weights
dat <- dat %>% 
  mutate(weight = if_else(treatment == 1, 1/(ps), 1/(1-ps)))

# apply weights to data
{{weighteddata <- svydesign(ids = ~1, data = dat, weights = ~ weight)}}

# weighted table 1

weightedtable <- svyCreateTableOne(vars = xvars, strata = "treatment", 
                                   data = weighteddata, test = FALSE)

```
]

.small[
```{r, eval=FALSE}
## Show table with SMD

print(weightedtable, smd = TRUE)
```
]


---
class: middle
## Check balance

- numbers in the brackets `()` are standard deviations but we can ignore them.

.small[
```r
                    Stratified by treatment
                      0               1               SMD   
  n                   5760.80         5660.88               
  ARF (mean (SD))        0.43 (0.50)     0.45 (0.50)   0.021
  CHF (mean (SD))        0.08 (0.27)     0.08 (0.27)   0.001
  Cirr (mean (SD))       0.04 (0.19)     0.04 (0.18)   0.017
  colcan (mean (SD))     0.00 (0.04)     0.00 (0.06)   0.039
  Coma (mean (SD))       0.07 (0.26)     0.07 (0.25)   0.025
  lungcan (mean (SD))    0.01 (0.08)     0.01 (0.08)   0.010
  MOSF (mean (SD))       0.07 (0.25)     0.07 (0.26)   0.008
  sepsis (mean (SD))     0.22 (0.41)     0.22 (0.41)   0.004
  age (mean (SD))       61.37 (17.59)   61.52 (15.22)  0.010
  female (mean (SD))     0.44 (0.50)     0.44 (0.50)  <0.001
  meanbp1 (mean (SD))   78.28 (38.20)   78.14 (38.34)  0.004
```
]

???

because these standard deviations are based on the weighted data, 
which shows as a weighted sample size. They are not real sample size so sd does not really mean anything.

The means of the pseudo population is actually what we want. 

You can see the weighted mean are very well balanced.


---
class: middle 

## You can also get weighted mean in a hard way

$$\frac{\sum_{i = 1}^n I(A_i = 1) \frac{X_i}{\pi_i}}{\sum_{i = 1}^n \frac{I(A_i = 1)}{\pi_i}}$$

.small[
```{r}
dat %>% 
  dplyr::filter(treatment == 1) %>% 
  dplyr::select(treatment, weight, age) %>% 
  mutate(WtedAge = mean(weight*age)/mean(weight)) 
```
]

---
class: middle

## Causal relative risk 
  
.small[
```{r message=FALSE, warning=FALSE}
# get causal relative risk, use weighted GLM 
glm.obj <- glm(died ~ treatment, weights = weight, 
               data = dat, family = binomial(link = log))
# summary(glm.obj)
betaiptw <- coef(glm.obj)
# to properly account for weighting, use asymptotic (sandwich) variance

SE <- sqrt(diag(vcovHC(glm.obj, type = "HC0")))

# Get the point estimate and confidence intervals
causalrr <- exp(betaiptw[2])
lrr <- exp(betaiptw[2] - 1.96*SE[2])
urr <- exp(betaiptw[2] + 1.96*SE[2])
c(causalrr, lrr, urr)
```
]

???
because the weights in the `glm` function will make the sample size bigger than it actually was, so robust/sandwich covariance can help us to correct that. 

`vcovHC` gives us robust variance-covariance matrix

`diag` keep the numbers in the diagonal 

then take square root to calculate the standard error.

---
class: middle

## Causal risk difference 

.small[
```{r message=FALSE, warning=FALSE}
# get causal risk difference, use weighted GLM 
glm.obj <- glm(died ~ treatment, weights = weight, 
               data = dat, family = binomial(link = "identity"))
# summary(glm.obj)
betaiptw <- coef(glm.obj)
SE <- sqrt(diag(vcovHC(glm.obj, type = "HC0")))

# causal risk difference
causalrd <- betaiptw[2]
lrd <- betaiptw[2]  - 1.96*SE[2]
urd <- betaiptw[2]  + 1.96*SE[2]
c(causalrd, lrd, urd)
```
]

---
class: middle

## Let's repeat using the `ipw` package

.small[
```{r}
weightmodel <- ipwpoint(exposure = treatment, 
                        family = "binomial",
                        link = "logit",
                        denominator = ~ age + female + meanbp1 + ARF +
                          CHF + Cirr + colcan + Coma + lungcan + MOSF + 
                          sepsis + aps, data = dat)

summary(weightmodel$ipw.weights)
```
]


.small[
```{r eval=FALSE}
# get the density plot easily
ipwplot(weights = weightmodel$ipw.weights, logscale = FALSE,
        main = "Weights", xlim = c(0, 18))
```

]

---
class: inverse
background-image: url("./fig/densityplotWeights.png")
background-position: 50% 50%
background-size: contain

???
this figure is showing the distribution of the weights. 


---
class: middle

## Get causal risk difference 

.small[
```{r}
# causal risk difference using survey designed glm
svyglm.obj <- (svyglm(died ~ treatment, 
                      design = svydesign(~ 1, weights = ~weight, data = dat)))

coef(svyglm.obj)

confint(svyglm.obj)
```
]

---
class: middle

## Truncated weights 

.small[
```r
weight <- dat$weight
truncweight <- replace(weight, weight > 15, 15)
# causal risk difference
glm.obj <- glm(died ~ treatment, weights = truncweight, 
               family = binomial(link = "identity"), 
               data = dat)

# truncated weights use ipw package
weightmodel <- ipwpoint(exposure = treatment, family = "binomial",
                        link = "logit", 
                        denominator = ~ age + female + meanbp1 + ARF +
                          CHF + Cirr + colcan + Coma + lungcan + MOSF + 
                          sepsis + aps, data = dat, 
*                       trunc = 0.01)

```
]

---
class: middle

## Truncate by percentile

.small[
```{r echo=FALSE}
weightmodel <- ipwpoint(exposure = treatment, family = "binomial",
                        link = "logit", 
                        denominator = ~ age + female + meanbp1 + ARF +
                          CHF + Cirr + colcan + Coma + lungcan + MOSF + 
                          sepsis + aps, data = dat, 
                       trunc = 0.01)
```
]

.small[
```{r}
summary(weightmodel$weights.trun)
```
]


.small[
```{r}
# get risk difference
dat$wt <- weightmodel$weights.trun
svyglm.obj <- (svyglm(died ~ treatment, 
                      design = svydesign(~ 1, weights = ~wt, data = dat)))
coef(svyglm.obj)
confint(svyglm.obj)
```
]

---
class: inverse, bottom
background-image: url("./fig/truncatedWeights.png")
background-position: 50% 50%
background-size: contain


.small[
```{r eval=FALSE, echo=FALSE}
# plot of weights
ipwplot(weights = weightmodel$weights.trun, logscale = FALSE,
        main = "truncated Weights", xlim = c(0, 7))
```
]


---
class: inverse, center, middle

# Test for unobserved covariates

---
class: middle

## Example 1 (Correct model)

.small[
```{r}
psmodel <- glm(treatment ~ age + female + meanbp1 + ARF + 
                 CHF + Cirr + colcan + Coma + lungcan + 
                 MOSF + sepsis + aps, family = binomial(link = "logit"), 
               data = dat)

ps <- predict(psmodel, type = "response")

dat["residuals"] <- residuals.glm(psmodel)

dat <- dat %>%
  mutate(id_num = 1:n())

m.out <- MatchIt::matchit(treatment ~ age + female + meanbp1 + ARF + 
                 CHF + Cirr + colcan + Coma + lungcan + 
                 MOSF + sepsis + aps, data = dat, 
                 method = "nearest", distance = "logit", caliper = 0.2)

```
]


---
class: middle

## Manipulation

.small[
```{r }
a <- data.frame(dat$id_num, m.out$treat, m.out$weights)
colnames(a) <- c("id_num", "trt", "weights")

# datafile of matches
b <- as.data.frame(m.out$match.matrix)
colnames(b) <- c("matched_unit")
b$matched_unit <- as.numeric(as.character(b$matched_unit))
b$treated_unit <- as.numeric(rownames(b))
# now delete matches=na
c <- b[!is.na(b$matched_unit), ]
c$match_num <- 1:dim(c)[1]

# attach match number to large datafile
a[c$matched_unit, 4] <- c$match_num
a[c$treated_unit, 4] <- c$match_num
colnames(a)[4] <- "match_num"

final_dat <- dat %>%
  dplyr::select(-id_num) %>%
  cbind(a) %>%
  filter(!(is.na(a$match_num)))
```
]


---
class: middle

## Hausman test

.small[
```{r, message=FALSE, warning=FALSE, eval=FALSE}
# Hausman test

final_dat <- final_dat %>%
  dplyr::mutate(dead = if_else(death == "No", 0, 1))

outcomemodel <- survival::clogit(dead ~ age + female + meanbp1 + ARF +
  CHF + Cirr + Coma + lungcan +
  MOSF + sepsis + aps + strata(match_num) + residuals,
data = final_dat)

covariance <- diag(vcov(outcomemodel))
covariance <- covariance[length(covariance)]

coefficient <- coef(outcomemodel)[length(coef(outcomemodel))]

z_stat <- coefficient / sqrt(covariance)
p_values <- pchisq(z_stat^2, 1, lower.tail = FALSE)
names(p_values)=NULL
cat(paste0("P value: ",round(p_values,digit=4)))
```
]

.small[
```
# P value: 0.8558
```
]

???
We cannot reject the null hypothesis, and hence we cannot invalidate the absence of confounders.


---
class: middle 

## Example 2 (Residual confounders - Extreme example)

.small[

```{r}
#Propensity score
psmodel <- glm(treatment ~ age,
                  family = binomial(link = "logit"),
                  data = dat)

ps <- predict(psmodel, type = "response")

dat["residuals"] <- residuals.glm(psmodel)

dat <- dat %>%
  mutate(id_num = 1:n())

m.out <- MatchIt::matchit(treatment ~ age,
data = dat, method = "nearest", distance = "logit", caliper = 0.2
)
```
]


---
class: middle

## Manipulation

.small[
```{r }
a <- data.frame(dat$id_num, m.out$treat, m.out$weights)
colnames(a) <- c("id_num", "trt", "weights")

# datafile of matches
b <- as.data.frame(m.out$match.matrix)
colnames(b) <- c("matched_unit")
b$matched_unit <- as.numeric(as.character(b$matched_unit))
b$treated_unit <- as.numeric(rownames(b))
# now delete matches=na
c <- b[!is.na(b$matched_unit), ]
c$match_num <- 1:dim(c)[1]

# attach match number to large datafile
a[c$matched_unit, 4] <- c$match_num
a[c$treated_unit, 4] <- c$match_num
colnames(a)[4] <- "match_num"

final_dat <- dat %>%
  dplyr::select(-id_num) %>%
  cbind(a) %>%
  filter(!(is.na(a$match_num)))
```
]

---
class: middle

## Hausman test

.small[
```{r,message=FALSE,warning=FALSE, eval=FALSE}
# Hausman test
library(survival)
final_dat <- final_dat %>%
  mutate(dead = ifelse(death == "No", 0, 1))

outcomemodel <- survival::clogit(dead ~ age + strata(match_num) + residuals,
data = final_dat
)

covariance <- diag(vcov(outcomemodel))
covariance <- covariance[length(covariance)]

coefficient <- coef(outcomemodel)[length(coef(outcomemodel))]

z_stat <- coefficient / sqrt(covariance)
p_values <- pchisq(z_stat^2, 1, lower.tail = FALSE)
names(p_values)=NULL

cat(paste0("P value: ",formatC(p_values,format="e",digits=2)))
```
]

.small[
```
# P value: 1.93e-02
```
]


???
In this case, we reject the null hypothesis $H_0:\delta=0$, and conclude about the presence of confounders.

---
class: middle 

# Reading list

1. 5 misunderstandings about propensity score <br> [PSに関する5つの誤解](https://healthpolicyhealthecon.com/2015/05/07/propensity-score-2/)

2. Causal inference in Statistics,  [統計学における因果推論（ルービンの因果モデル）](https://healthpolicyhealthecon.com/2014/11/30/rubin_causal_model/)

3. Tableone package has many useful code for tables and figures: [https://cran.r-project.org/web/packages/tableone/vignettes/smd.html](https://cran.r-project.org/web/packages/tableone/vignettes/smd.html)

4. `rcbalance` package has many more matching options: [https://obsstudies.org/wp-content/uploads/2017/06/rcbalance_paper_v7r2.pdf](https://obsstudies.org/wp-content/uploads/2017/06/rcbalance_paper_v7r2.pdf)
